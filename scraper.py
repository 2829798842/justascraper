#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å—åç§°ï¼šscraper
@Time: 2025-01-20 16:06:00
@Author: Yang208115
@File: scraper.py
@Desc: é€šçŸ¥å…¬å‘Šçš„è‡ªåŠ¨çˆ¬å–ã€è§£æå’Œæ–°é€šçŸ¥æ£€æµ‹åŠŸèƒ½
"""

import requests
import json
import os
import sys
import time
import logging
from datetime import datetime
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from config import TARGET_URL, REQUEST_CONFIG, STORAGE_CONFIG, SCRAPE_CONFIG, LOGGING_CONFIG
from utils.notifier import MessageNotifier


class QingdaoHRSScraper:
    """
    Attributes:
        target_url (str): ç›®æ ‡ç½‘ç«™URL
        session (requests.Session): HTTPä¼šè¯å¯¹è±¡
        data_file (str): æ•°æ®å­˜å‚¨æ–‡ä»¶è·¯å¾„
        logger (logging.Logger): æ—¥å¿—è®°å½•å™¨
    """
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«ã€‚"""
        self.target_url = TARGET_URL
        self.session = requests.Session()
        self.session.headers.update(REQUEST_CONFIG["headers"])
        self.data_file = STORAGE_CONFIG["data_file"]
        self.notifier = MessageNotifier()
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=getattr(logging, LOGGING_CONFIG["level"]),
            format=LOGGING_CONFIG["format"],
            handlers=[
                logging.FileHandler(STORAGE_CONFIG["log_file"], encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def fetch_page_content(self) -> Optional[str]:
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            self.logger.info(f"æ­£åœ¨è®¿é—®: {self.target_url}")
            
            response = self.session.get(
                self.target_url,
                timeout=REQUEST_CONFIG["timeout"]
            )
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            self.logger.info(f"æˆåŠŸè·å–ç½‘é¡µå†…å®¹ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return response.text
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥: {e}")
            return None
            
    def parse_announcements(self, html_content: str) -> List[Dict]:
        """è§£æé€šçŸ¥å…¬å‘Šä¿¡æ¯"""
        try:
            soup = BeautifulSoup(html_content, 'lxml')
            announcements = []
            
            # æ ¹æ®ç½‘é¡µç»“æ„æŸ¥æ‰¾é€šçŸ¥å…¬å‘Šé“¾æ¥
            # éœ€è¦åˆ†æå…·ä½“çš„HTMLç»“æ„æ¥ç¡®å®šé€‰æ‹©å™¨
            self.logger.info("å¼€å§‹è§£æé€šçŸ¥å…¬å‘Šä¿¡æ¯...")
            
            # æš‚æ—¶ä½¿ç”¨é€šç”¨çš„é“¾æ¥æŸ¥æ‰¾æ–¹æ³•
            # å®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®é¡µé¢ç»“æ„è°ƒæ•´é€‰æ‹©å™¨
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link.get('href')
                text = link.get_text(strip=True)
                
                # è¿‡æ»¤å‡ºé€šçŸ¥å…¬å‘Šç›¸å…³é“¾æ¥
                if self.is_announcement_link(text, href):
                    announcement = {
                        'id': self.generate_id(text, href),
                        'title': text,
                        'url': self.get_full_url(href),
                        'scraped_at': datetime.now().strftime(SCRAPE_CONFIG["date_format"]),
                        'is_new': True
                    }
                    announcements.append(announcement)
                    
            self.logger.info(f"è§£æå®Œæˆï¼Œæ‰¾åˆ° {len(announcements)} æ¡é€šçŸ¥")
            return announcements
            
        except Exception as e:
            self.logger.error(f"è§£æé€šçŸ¥å…¬å‘Šå¤±è´¥: {e}")
            return []
            
    def is_announcement_link(self, text: str, href: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé€šçŸ¥å…¬å‘Šé“¾æ¥"""
        if not text or not href:
            return False
            
        # é€šçŸ¥å…¬å‘Šçš„å…³é”®è¯
        keywords = ['é€šçŸ¥', 'å…¬å‘Š', 'å…³äº', 'èŒç§°', 'è¯„å®¡', 'æŠ¥é€']
        
        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬æ˜¯å¦åŒ…å«å…³é”®è¯
        return any(keyword in text for keyword in keywords)
        
    def generate_id(self, title: str, url: str) -> str:
        """ç”Ÿæˆé€šçŸ¥çš„å”¯ä¸€ID"""
        import hashlib
        content = f"{title}_{url}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:16]
        
    def get_full_url(self, href: str) -> str:
        """è·å–å®Œæ•´URL"""
        if href.startswith('http'):
            return href
        elif href.startswith('/'):
            return f"https://hrss.qingdao.gov.cn{href}"
        else:
            return f"https://hrss.qingdao.gov.cn/ztzl_47/zcpd_47/tzgg_47/{href}"
            
    def load_existing_data(self) -> Dict:
        """åŠ è½½å·²å­˜åœ¨çš„æ•°æ®"""
        if os.path.exists(self.data_file):
            try:
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.error(f"åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        
        # è¿”å›é»˜è®¤ç»“æ„
        return {
            "announcements": [],
            "last_check": "",
            "total_count": 0
        }
        
    def save_data(self, data: Dict):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            os.makedirs(os.path.dirname(self.data_file), exist_ok=True)
            with open(self.data_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            self.logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {self.data_file}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            
    def detect_new_announcements(self, current_announcements: List[Dict], existing_data: Dict) -> List[Dict]:
        """æ£€æµ‹æ–°é€šçŸ¥"""
        existing_ids = {ann['id'] for ann in existing_data.get('announcements', [])}
        new_announcements = []
        
        for announcement in current_announcements:
            if announcement['id'] not in existing_ids:
                new_announcements.append(announcement)
                
        return new_announcements
        
    def notify_new_announcements(self, new_announcements: List[Dict]):
        """é€šçŸ¥æ–°çš„å…¬å‘Šã€‚
        
        Args:
            new_announcements: æ–°é€šçŸ¥å…¬å‘Šåˆ—è¡¨
        """
        if not new_announcements:
            self.logger.info("æ²¡æœ‰å‘ç°æ–°çš„é€šçŸ¥å…¬å‘Š")
            return
            
        self.logger.info(f"ğŸ”” å‘ç° {len(new_announcements)} æ¡æ–°é€šçŸ¥:")
        for announcement in new_announcements:
            self.logger.info(f"- {announcement['title']}")
            self.logger.info(f"  é“¾æ¥: {announcement['url']}")
            
        # å‘é€æ¨é€é€šçŸ¥
        try:
            if self.notifier.send_notification(new_announcements):
                self.logger.info("ğŸ“± æ¨é€é€šçŸ¥å‘é€æˆåŠŸ")
            else:
                self.logger.warning("âš ï¸ æ¨é€é€šçŸ¥å‘é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥é€šçŸ¥é…ç½®")
        except Exception as e:
            self.logger.error(f"âŒ æ¨é€é€šçŸ¥å‡ºé”™: {e}")
        
    def run_once(self):
        """æ‰§è¡Œä¸€æ¬¡çˆ¬å–ä»»åŠ¡"""
        self.logger.info("å¼€å§‹æ‰§è¡Œçˆ¬å–ä»»åŠ¡...")
        
        # è·å–ç½‘é¡µå†…å®¹
        html_content = self.fetch_page_content()
        if not html_content:
            self.logger.error("æ— æ³•è·å–ç½‘é¡µå†…å®¹ï¼Œè·³è¿‡æœ¬æ¬¡æ£€æŸ¥")
            return
            
        # è§£æé€šçŸ¥å…¬å‘Š
        current_announcements = self.parse_announcements(html_content)
        if not current_announcements:
            self.logger.warning("æœªè§£æåˆ°ä»»ä½•é€šçŸ¥å…¬å‘Š")
            
        # åŠ è½½ç°æœ‰æ•°æ®
        existing_data = self.load_existing_data()
        
        # æ£€æµ‹æ–°é€šçŸ¥
        new_announcements = self.detect_new_announcements(current_announcements, existing_data)
        
        # é€šçŸ¥æ–°å…¬å‘Š
        self.notify_new_announcements(new_announcements)
        
        # æ›´æ–°æ•°æ®
        all_announcements = existing_data.get('announcements', [])
        
        # æ·»åŠ æ–°é€šçŸ¥
        for new_ann in new_announcements:
            all_announcements.append(new_ann)
            
        # é™åˆ¶å­˜å‚¨æ•°é‡
        max_count = SCRAPE_CONFIG["max_announcements"]
        if len(all_announcements) > max_count:
            all_announcements = all_announcements[-max_count:]
            
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        updated_data = {
            "announcements": all_announcements,
            "last_check": datetime.now().strftime(SCRAPE_CONFIG["date_format"]),
            "total_count": len(all_announcements)
        }
        self.save_data(updated_data)
        
        self.logger.info("çˆ¬å–ä»»åŠ¡å®Œæˆ")
        
    def run_daemon(self):
        """ä»¥å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼è¿è¡Œã€‚
        
        é•¿æœŸè¿è¡Œç›‘æ§ç¨‹åºï¼Œå®šæœŸæ£€æŸ¥æ–°é€šçŸ¥å¹¶æ¨é€æ¶ˆæ¯ã€‚
        """
        self.logger.info("ğŸš€ å¯åŠ¨å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼...")
        self.logger.info(f"â° æ£€æŸ¥é—´éš”: {SCRAPE_CONFIG['check_interval']} ç§’ ({SCRAPE_CONFIG['check_interval']//60} åˆ†é’Ÿ)")
        self.logger.info("ï¿½ Windowsç³»ç»Ÿé€šçŸ¥: å·²å¯ç”¨")
        self.logger.info("ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
        
        # å‘é€å¯åŠ¨é€šçŸ¥
        try:
            self.notifier.send_windows_notification(
                "é’å²›äººç¤¾å±€é€šçŸ¥ç›‘æ§", 
                "ç›‘æ§ç³»ç»Ÿå·²å¯åŠ¨ï¼Œå°†å®šæœŸæ£€æŸ¥æ–°é€šçŸ¥"
            )
        except Exception:
            pass  # å¿½ç•¥é€šçŸ¥å¤±è´¥
        
        error_count = 0
        max_errors = 5  # æœ€å¤§è¿ç»­é”™è¯¯æ¬¡æ•°
        
        while True:
            try:
                self.run_once()
                error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡æ£€æŸ¥
                self.logger.info(f"ğŸ˜´ ç­‰å¾… {SCRAPE_CONFIG['check_interval']} ç§’åè¿›è¡Œä¸‹æ¬¡æ£€æŸ¥...")
                time.sleep(SCRAPE_CONFIG["check_interval"])
                
            except KeyboardInterrupt:
                self.logger.info("ğŸ‘‹ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
                # å‘é€åœæ­¢é€šçŸ¥
                try:
                    self.notifier.send_windows_notification(
                        "é’å²›äººç¤¾å±€é€šçŸ¥ç›‘æ§", 
                        "ç›‘æ§ç³»ç»Ÿå·²åœæ­¢"
                    )
                except Exception:
                    pass
                break
            except requests.exceptions.RequestException as e:
                error_count += 1
                self.logger.error(f"ğŸŒ ç½‘ç»œè¯·æ±‚é”™è¯¯ ({error_count}/{max_errors}): {e}")
                
                if error_count >= max_errors:
                    self.logger.error("âŒ è¿ç»­ç½‘ç»œé”™è¯¯è¿‡å¤šï¼Œç¨‹åºé€€å‡º")
                    break
                    
                # ç½‘ç»œé”™è¯¯æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                time.sleep(min(300, 60 * error_count))  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
                
            except Exception as e:
                error_count += 1
                self.logger.error(f"âš ï¸ è¿è¡Œæ—¶é”™è¯¯ ({error_count}/{max_errors}): {e}")
                
                if error_count >= max_errors:
                    self.logger.error("âŒ è¿ç»­é”™è¯¯è¿‡å¤šï¼Œç¨‹åºé€€å‡º")
                    break
                    
                time.sleep(60)  # å…¶ä»–é”™è¯¯ç­‰å¾…1åˆ†é’Ÿ


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # å…ˆè¿è¡Œå¼€æœºè‡ªæ£€ï¼ˆé™é»˜æ¨¡å¼ï¼‰
    print("ğŸ”§ æ­£åœ¨è¿›è¡Œç³»ç»Ÿè‡ªæ£€...")
    try:
        from startup_check import run_startup_check
        if not run_startup_check(silent=True):
            print("âŒ ç³»ç»Ÿè‡ªæ£€å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            print("ğŸ’¡ è¿è¡Œ 'python startup_check.py' æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
            return
    except Exception as e:
        print(f"âŒ è‡ªæ£€è¿‡ç¨‹å‡ºé”™: {e}")
        return
    
    print("âœ… ç³»ç»Ÿè‡ªæ£€é€šè¿‡ï¼Œå¯åŠ¨çˆ¬è™«ç¨‹åº...")
    print("-" * 50)
    
    parser = argparse.ArgumentParser(description='é€šçŸ¥å…¬å‘Šçˆ¬è™«')
    parser.add_argument('--daemon', action='store_true', help='ä»¥å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼è¿è¡Œ')
    parser.add_argument('--once', action='store_true', help='åªæ‰§è¡Œä¸€æ¬¡çˆ¬å–')
    
    args = parser.parse_args()
    
    scraper = QingdaoHRSScraper()
    
    if args.daemon:
        scraper.run_daemon()
    else:
        scraper.run_once()


if __name__ == "__main__":
    main()